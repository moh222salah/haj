"""
Hajj & Umrah Analytics Platform
================================
Ù…Ù†ØµØ© ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø¬ ÙˆØ§Ù„Ø¹Ù…Ø±Ø© Ù„Ù„Ø³ÙˆÙ‚ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠ

Features:
- Privacy-compliant decorators
- Efficient data processing with generators
- Parallel analysis with multithreading
- Real-time analytics and reporting

Author: Your Name
License: MIT
"""

import functools
import time
import hashlib
import logging
from datetime import datetime, timedelta
from typing import Generator, Callable, Any, Dict, List
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from enum import Enum
import random
import json

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)


# ==================== DECORATORS ====================

def privacy_compliance(func: Callable) -> Callable:
    """
    Decorator: ØªØ·Ø¨ÙŠÙ‚ Ø³ÙŠØ§Ø³Ø§Øª Ø§Ù„Ø®ØµÙˆØµÙŠØ© Ø¹Ù„Ù‰ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø©
    ÙŠÙ‚ÙˆÙ… Ø¨ØªØ´ÙÙŠØ± Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ø´Ø®ØµÙŠØ© Ù‚Ø¨Ù„ Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø©
    """
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        logger.info(f"ğŸ”’ Privacy check: {func.__name__}")
        
        # ØªØ´ÙÙŠØ± Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø³Ø§Ø³Ø©
        if args and isinstance(args[0], dict):
            data = args[0].copy()
            sensitive_fields = ['national_id', 'passport_number', 'phone']
            
            for field in sensitive_fields:
                if field in data:
                    original = data[field]
                    data[field] = hashlib.sha256(str(original).encode()).hexdigest()[:16]
            
            return func(data, *args[1:], **kwargs)
        
        return func(*args, **kwargs)
    
    return wrapper


def performance_monitor(func: Callable) -> Callable:
    """
    Decorator: Ù…Ø±Ø§Ù‚Ø¨Ø© Ø£Ø¯Ø§Ø¡ Ø§Ù„Ø¯ÙˆØ§Ù„ ÙˆØªØ³Ø¬ÙŠÙ„ Ø§Ù„ÙˆÙ‚Øª Ø§Ù„Ù…Ø³ØªØºØ±Ù‚
    """
    @functools.wraps(func)
    def wrapper(*args, **kwargs):
        start_time = time.time()
        result = func(*args, **kwargs)
        elapsed_time = time.time() - start_time
        
        logger.info(f"â±ï¸  {func.__name__} took {elapsed_time:.4f} seconds")
        return result
    
    return wrapper


def retry_on_failure(max_retries: int = 3, delay: float = 1.0):
    """
    Decorator: Ø¥Ø¹Ø§Ø¯Ø© Ø§Ù„Ù…Ø­Ø§ÙˆÙ„Ø© Ø¹Ù†Ø¯ Ø§Ù„ÙØ´Ù„ (Ù„Ù„Ø¹Ù…Ù„ÙŠØ§Øª Ø§Ù„Ø­Ø±Ø¬Ø©)
    """
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            for attempt in range(max_retries):
                try:
                    return func(*args, **kwargs)
                except Exception as e:
                    if attempt == max_retries - 1:
                        logger.error(f"âŒ Failed after {max_retries} attempts: {e}")
                        raise
                    logger.warning(f"âš ï¸  Attempt {attempt + 1} failed, retrying...")
                    time.sleep(delay)
            return None
        return wrapper
    return decorator


def cache_results(ttl_seconds: int = 300):
    """
    Decorator: ØªØ®Ø²ÙŠÙ† Ù…Ø¤Ù‚Øª Ù„Ù„Ù†ØªØ§Ø¦Ø¬ Ù„ØªØ­Ø³ÙŠÙ† Ø§Ù„Ø£Ø¯Ø§Ø¡
    """
    cache = {}
    
    def decorator(func: Callable) -> Callable:
        @functools.wraps(func)
        def wrapper(*args, **kwargs):
            key = str(args) + str(kwargs)
            current_time = time.time()
            
            if key in cache:
                result, timestamp = cache[key]
                if current_time - timestamp < ttl_seconds:
                    logger.info(f"ğŸ“¦ Cache hit for {func.__name__}")
                    return result
            
            result = func(*args, **kwargs)
            cache[key] = (result, current_time)
            return result
        
        return wrapper
    return decorator


# ==================== ENUMS & DATA CLASSES ====================

class PilgrimType(Enum):
    """Ù†ÙˆØ¹ Ø§Ù„Ø²Ø§Ø¦Ø±"""
    HAJJ = "Ø­Ø¬"
    UMRAH = "Ø¹Ù…Ø±Ø©"


class Nationality(Enum):
    """Ø§Ù„Ø¬Ù†Ø³ÙŠØ§Øª Ø§Ù„Ø´Ø§Ø¦Ø¹Ø©"""
    SAUDI = "Ø³Ø¹ÙˆØ¯ÙŠ"
    EGYPTIAN = "Ù…ØµØ±ÙŠ"
    PAKISTANI = "Ø¨Ø§ÙƒØ³ØªØ§Ù†ÙŠ"
    INDONESIAN = "Ø¥Ù†Ø¯ÙˆÙ†ÙŠØ³ÙŠ"
    INDIAN = "Ù‡Ù†Ø¯ÙŠ"
    BANGLADESHI = "Ø¨Ù†Ø¬Ù„Ø§Ø¯ÙŠØ´ÙŠ"
    TURKISH = "ØªØ±ÙƒÙŠ"
    OTHER = "Ø£Ø®Ø±Ù‰"


@dataclass
class PilgrimRecord:
    """Ø³Ø¬Ù„ Ø­Ø§Ø¬ Ø£Ùˆ Ù…Ø¹ØªÙ…Ø±"""
    id: str
    national_id: str
    passport_number: str
    name: str
    age: int
    gender: str
    nationality: Nationality
    phone: str
    pilgrim_type: PilgrimType
    arrival_date: datetime
    departure_date: datetime
    accommodation_id: str
    transport_id: str
    health_status: str
    
    def to_dict(self) -> Dict:
        return {
            'id': self.id,
            'national_id': self.national_id,
            'passport_number': self.passport_number,
            'name': self.name,
            'age': self.age,
            'gender': self.gender,
            'nationality': self.nationality.value,
            'phone': self.phone,
            'pilgrim_type': self.pilgrim_type.value,
            'arrival_date': self.arrival_date.isoformat(),
            'departure_date': self.departure_date.isoformat(),
            'accommodation_id': self.accommodation_id,
            'transport_id': self.transport_id,
            'health_status': self.health_status
        }


# ==================== GENERATORS ====================

def generate_synthetic_pilgrims(count: int) -> Generator[PilgrimRecord, None, None]:
    """
    Generator: ØªÙˆÙ„ÙŠØ¯ Ø¨ÙŠØ§Ù†Ø§Øª ØªØ¬Ø±ÙŠØ¨ÙŠØ© Ù„Ù„Ø­Ø¬Ø§Ø¬ ÙˆØ§Ù„Ù…Ø¹ØªÙ…Ø±ÙŠÙ†
    ÙŠØ³ØªØ®Ø¯Ù… Generator Ù„ØªÙˆÙÙŠØ± Ø§Ù„Ø°Ø§ÙƒØ±Ø© Ø¹Ù†Ø¯ Ù…Ø¹Ø§Ù„Ø¬Ø© Ù…Ù„Ø§ÙŠÙŠÙ† Ø§Ù„Ø³Ø¬Ù„Ø§Øª
    """
    logger.info(f"ğŸ”„ Generating {count} synthetic pilgrim records...")
    
    names = ["Ù…Ø­Ù…Ø¯", "Ø£Ø­Ù…Ø¯", "ÙØ§Ø·Ù…Ø©", "Ø¹Ø§Ø¦Ø´Ø©", "Ø¹Ø¨Ø¯Ø§Ù„Ù„Ù‡", "Ø³Ø§Ø±Ø©", "Ø®Ø§Ù„Ø¯", "Ù…Ø±ÙŠÙ…"]
    
    for i in range(count):
        arrival = datetime.now() - timedelta(days=random.randint(1, 30))
        departure = arrival + timedelta(days=random.randint(5, 15))
        
        record = PilgrimRecord(
            id=f"PIL{i:08d}",
            national_id=f"{random.randint(1000000000, 9999999999)}",
            passport_number=f"P{random.randint(10000000, 99999999)}",
            name=random.choice(names),
            age=random.randint(18, 80),
            gender=random.choice(["Ø°ÙƒØ±", "Ø£Ù†Ø«Ù‰"]),
            nationality=random.choice(list(Nationality)),
            phone=f"+966{random.randint(500000000, 599999999)}",
            pilgrim_type=random.choice(list(PilgrimType)),
            arrival_date=arrival,
            departure_date=departure,
            accommodation_id=f"ACC{random.randint(1000, 9999)}",
            transport_id=f"TRN{random.randint(100, 999)}",
            health_status=random.choice(["Ø¬ÙŠØ¯", "Ù…Ù…ØªØ§Ø²", "ÙŠØ­ØªØ§Ø¬ Ù…ØªØ§Ø¨Ø¹Ø©"])
        )
        
        yield record
        
        # Ø¹Ø±Ø¶ Ø§Ù„ØªÙ‚Ø¯Ù… ÙƒÙ„ 10000 Ø³Ø¬Ù„
        if (i + 1) % 10000 == 0:
            logger.info(f"  Generated {i + 1:,} records...")


def stream_time_series_analysis(
    records: List[PilgrimRecord],
    chunk_size: int = 1000
) -> Generator[Dict[str, Any], None, None]:
    """
    Generator: ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ© Ø¨Ø´ÙƒÙ„ Ù…ØªØ¯ÙÙ‚
    Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¹Ù„Ù‰ Ø¯ÙØ¹Ø§Øª Ù„ØªØ¬Ù†Ø¨ Ø§Ø³ØªÙ‡Ù„Ø§Ùƒ Ø§Ù„Ø°Ø§ÙƒØ±Ø©
    """
    logger.info("ğŸ“Š Starting time-series analysis...")
    
    for i in range(0, len(records), chunk_size):
        chunk = records[i:i + chunk_size]
        
        # ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø¯ÙØ¹Ø©
        analysis = {
            'chunk_id': i // chunk_size + 1,
            'chunk_size': len(chunk),
            'date_range': {
                'start': min(r.arrival_date for r in chunk).isoformat(),
                'end': max(r.departure_date for r in chunk).isoformat()
            },
            'statistics': {
                'total_pilgrims': len(chunk),
                'avg_age': sum(r.age for r in chunk) / len(chunk),
                'male_count': sum(1 for r in chunk if r.gender == "Ø°ÙƒØ±"),
                'female_count': sum(1 for r in chunk if r.gender == "Ø£Ù†Ø«Ù‰"),
                'hajj_count': sum(1 for r in chunk if r.pilgrim_type == PilgrimType.HAJJ),
                'umrah_count': sum(1 for r in chunk if r.pilgrim_type == PilgrimType.UMRAH),
            }
        }
        
        yield analysis


def filter_by_criteria(
    records: Generator[PilgrimRecord, None, None],
    criteria: Dict[str, Any]
) -> Generator[PilgrimRecord, None, None]:
    """
    Generator: ØªØµÙÙŠØ© Ø§Ù„Ø³Ø¬Ù„Ø§Øª Ø­Ø³Ø¨ Ù…Ø¹Ø§ÙŠÙŠØ± Ù…Ø­Ø¯Ø¯Ø©
    """
    logger.info(f"ğŸ” Filtering records with criteria: {criteria}")
    
    for record in records:
        match = True
        
        if 'nationality' in criteria:
            match = match and record.nationality == criteria['nationality']
        
        if 'min_age' in criteria:
            match = match and record.age >= criteria['min_age']
        
        if 'max_age' in criteria:
            match = match and record.age <= criteria['max_age']
        
        if 'pilgrim_type' in criteria:
            match = match and record.pilgrim_type == criteria['pilgrim_type']
        
        if match:
            yield record


# ==================== MULTITHREADING ====================

class DataAnalyzer:
    """Ù…Ø­Ù„Ù„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ø¹ Ø¯Ø¹Ù… Ø§Ù„Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠØ©"""
    
    def __init__(self, max_workers: int = 4):
        self.max_workers = max_workers
        self.executor = ThreadPoolExecutor(max_workers=max_workers)
    
    @performance_monitor
    def analyze_by_nationality(self, records: List[PilgrimRecord]) -> Dict[str, int]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø­Ø³Ø¨ Ø§Ù„Ø¬Ù†Ø³ÙŠØ©"""
        logger.info("ğŸŒ Analyzing nationality distribution...")
        
        nationality_count = {}
        for record in records:
            nat = record.nationality.value
            nationality_count[nat] = nationality_count.get(nat, 0) + 1
        
        return nationality_count
    
    @performance_monitor
    def analyze_age_groups(self, records: List[PilgrimRecord]) -> Dict[str, int]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„ØªÙˆØ²ÙŠØ¹ Ø§Ù„Ø¹Ù…Ø±ÙŠ"""
        logger.info("ğŸ‘¥ Analyzing age group distribution...")
        
        age_groups = {
            '18-30': 0,
            '31-45': 0,
            '46-60': 0,
            '60+': 0
        }
        
        for record in records:
            if record.age <= 30:
                age_groups['18-30'] += 1
            elif record.age <= 45:
                age_groups['31-45'] += 1
            elif record.age <= 60:
                age_groups['46-60'] += 1
            else:
                age_groups['60+'] += 1
        
        return age_groups
    
    @performance_monitor
    def analyze_peak_periods(self, records: List[PilgrimRecord]) -> Dict[str, int]:
        """ØªØ­Ù„ÙŠÙ„ ÙØªØ±Ø§Øª Ø§Ù„Ø°Ø±ÙˆØ©"""
        logger.info("ğŸ“… Analyzing peak periods...")
        
        daily_arrivals = {}
        for record in records:
            date_key = record.arrival_date.strftime('%Y-%m-%d')
            daily_arrivals[date_key] = daily_arrivals.get(date_key, 0) + 1
        
        return daily_arrivals
    
    @performance_monitor
    @privacy_compliance
    def analyze_health_status(self, record: Dict) -> Dict[str, Any]:
        """ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø­Ø§Ù„Ø© Ø§Ù„ØµØ­ÙŠØ© (Ù…Ø¹ Ø­Ù…Ø§ÙŠØ© Ø§Ù„Ø®ØµÙˆØµÙŠØ©)"""
        return {
            'status': record.get('health_status', 'ØºÙŠØ± Ù…Ø­Ø¯Ø¯'),
            'requires_attention': record.get('health_status') == 'ÙŠØ­ØªØ§Ø¬ Ù…ØªØ§Ø¨Ø¹Ø©'
        }
    
    @performance_monitor
    def parallel_comprehensive_analysis(
        self,
        records: List[PilgrimRecord]
    ) -> Dict[str, Any]:
        """
        ØªØ­Ù„ÙŠÙ„ Ø´Ø§Ù…Ù„ Ù…ØªÙˆØ§Ø²ÙŠ Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Multithreading
        ÙŠÙ‚ÙˆÙ… Ø¨ØªØ´ØºÙŠÙ„ Ø¹Ø¯Ø© ØªØ­Ù„ÙŠÙ„Ø§Øª ÙÙŠ Ù†ÙØ³ Ø§Ù„ÙˆÙ‚Øª
        """
        logger.info(f"ğŸš€ Starting parallel analysis with {self.max_workers} workers...")
        
        futures = {
            'nationality': self.executor.submit(self.analyze_by_nationality, records),
            'age_groups': self.executor.submit(self.analyze_age_groups, records),
            'peak_periods': self.executor.submit(self.analyze_peak_periods, records)
        }
        
        results = {}
        for name, future in futures.items():
            try:
                results[name] = future.result()
                logger.info(f"  âœ… {name} analysis completed")
            except Exception as e:
                logger.error(f"  âŒ {name} analysis failed: {e}")
                results[name] = None
        
        return results
    
    def shutdown(self):
        """Ø¥ÙŠÙ‚Ø§Ù executor"""
        self.executor.shutdown(wait=True)


# ==================== MAIN APPLICATION ====================

class HajjUmrahAnalyticsPlatform:
    """Ø§Ù„Ù…Ù†ØµØ© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ© Ù„ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø¬ ÙˆØ§Ù„Ø¹Ù…Ø±Ø©"""
    
    def __init__(self):
        self.analyzer = DataAnalyzer(max_workers=4)
        self.records: List[PilgrimRecord] = []
    
    @retry_on_failure(max_retries=3, delay=1.0)
    @performance_monitor
    def load_data(self, count: int = 50000):
        """ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø¨Ø§Ø³ØªØ®Ø¯Ø§Ù… Generator"""
        logger.info(f"ğŸ“¥ Loading {count:,} pilgrim records...")
        
        # Ø§Ø³ØªØ®Ø¯Ø§Ù… Generator Ù„ØªÙˆÙ„ÙŠØ¯ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª
        generator = generate_synthetic_pilgrims(count)
        self.records = list(generator)
        
        logger.info(f"âœ… Successfully loaded {len(self.records):,} records")
    
    @cache_results(ttl_seconds=300)
    @performance_monitor
    def get_summary_statistics(self) -> Dict[str, Any]:
        """Ø§Ù„Ø­ØµÙˆÙ„ Ø¹Ù„Ù‰ Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ù…Ù„Ø®ØµØ©"""
        logger.info("ğŸ“ˆ Calculating summary statistics...")
        
        total = len(self.records)
        
        return {
            'total_pilgrims': total,
            'hajj_pilgrims': sum(1 for r in self.records if r.pilgrim_type == PilgrimType.HAJJ),
            'umrah_pilgrims': sum(1 for r in self.records if r.pilgrim_type == PilgrimType.UMRAH),
            'average_age': sum(r.age for r in self.records) / total if total > 0 else 0,
            'male_percentage': (sum(1 for r in self.records if r.gender == "Ø°ÙƒØ±") / total * 100) if total > 0 else 0,
            'female_percentage': (sum(1 for r in self.records if r.gender == "Ø£Ù†Ø«Ù‰") / total * 100) if total > 0 else 0,
        }
    
    def stream_analysis(self, chunk_size: int = 5000):
        """ØªØ­Ù„ÙŠÙ„ Ù…ØªØ¯ÙÙ‚ Ù„Ù„Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø²Ù…Ù†ÙŠØ©"""
        logger.info("ğŸŒŠ Starting streaming analysis...")
        
        for chunk_analysis in stream_time_series_analysis(self.records, chunk_size):
            logger.info(f"  Chunk {chunk_analysis['chunk_id']}: {chunk_analysis['statistics']['total_pilgrims']} records")
            yield chunk_analysis
    
    @performance_monitor
    def run_comprehensive_analysis(self) -> Dict[str, Any]:
        """ØªØ´ØºÙŠÙ„ Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„"""
        logger.info("ğŸ¯ Running comprehensive analysis...")
        
        # Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ
        parallel_results = self.analyzer.parallel_comprehensive_analysis(self.records)
        
        # Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù„Ø®ØµØ©
        summary = self.get_summary_statistics()
        
        # Ø¯Ù…Ø¬ Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        report = {
            'generated_at': datetime.now().isoformat(),
            'summary': summary,
            'detailed_analysis': parallel_results,
            'top_nationalities': dict(
                sorted(
                    parallel_results.get('nationality', {}).items(),
                    key=lambda x: x[1],
                    reverse=True
                )[:5]
            ) if parallel_results.get('nationality') else {}
        }
        
        return report
    
    def export_report(self, report: Dict[str, Any], filename: str = 'report.json'):
        """ØªØµØ¯ÙŠØ± Ø§Ù„ØªÙ‚Ø±ÙŠØ±"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(report, f, ensure_ascii=False, indent=2)
        logger.info(f"ğŸ“„ Report exported to {filename}")
    
    def cleanup(self):
        """ØªÙ†Ø¸ÙŠÙ Ø§Ù„Ù…ÙˆØ§Ø±Ø¯"""
        self.analyzer.shutdown()


# ==================== DEMO ====================

def main():
    """
    Ø§Ù„ØªØ·Ø¨ÙŠÙ‚ Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠ - Ø¹Ø±Ø¶ ØªÙˆØ¶ÙŠØ­ÙŠ
    """
    print("=" * 60)
    print("ğŸ•‹ Ù…Ù†ØµØ© ØªØ­Ù„ÙŠÙ„ Ø¨ÙŠØ§Ù†Ø§Øª Ø§Ù„Ø­Ø¬ ÙˆØ§Ù„Ø¹Ù…Ø±Ø©")
    print("   Hajj & Umrah Analytics Platform")
    print("=" * 60)
    print()
    
    # Ø¥Ù†Ø´Ø§Ø¡ Ø§Ù„Ù…Ù†ØµØ©
    platform = HajjUmrahAnalyticsPlatform()
    
    try:
        # 1. ØªØ­Ù…ÙŠÙ„ Ø§Ù„Ø¨ÙŠØ§Ù†Ø§Øª (Generator)
        print("ğŸ“Š Step 1: Loading data using Generators...")
        platform.load_data(count=50000)
        print()
        
        # 2. Ø§Ù„Ø¥Ø­ØµØ§Ø¦ÙŠØ§Øª Ø§Ù„Ù…Ù„Ø®ØµØ© (Cached Decorator)
        print("ğŸ“ˆ Step 2: Getting summary statistics (with caching)...")
        summary = platform.get_summary_statistics()
        print(f"   Total Pilgrims: {summary['total_pilgrims']:,}")
        print(f"   Hajj: {summary['hajj_pilgrims']:,} | Umrah: {summary['umrah_pilgrims']:,}")
        print(f"   Average Age: {summary['average_age']:.1f} years")
        print(f"   Gender: {summary['male_percentage']:.1f}% Male, {summary['female_percentage']:.1f}% Female")
        print()
        
        # 3. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ù…ØªØ¯ÙÙ‚ (Generator)
        print("ğŸŒŠ Step 3: Streaming analysis (first 3 chunks)...")
        for i, chunk in enumerate(platform.stream_analysis(chunk_size=10000)):
            if i >= 3:
                break
            print(f"   Chunk {chunk['chunk_id']}: {chunk['statistics']['total_pilgrims']} pilgrims analyzed")
        print()
        
        # 4. Ø§Ù„ØªØ­Ù„ÙŠÙ„ Ø§Ù„Ø´Ø§Ù…Ù„ Ø§Ù„Ù…ØªÙˆØ§Ø²ÙŠ (Multithreading)
        print("ğŸš€ Step 4: Running comprehensive parallel analysis...")
        report = platform.run_comprehensive_analysis()
        print()
        
        # 5. Ø¹Ø±Ø¶ Ø£Ù‡Ù… Ø§Ù„Ù†ØªØ§Ø¦Ø¬
        print("ğŸ† Top 5 Nationalities:")
        for nat, count in report['top_nationalities'].items():
            percentage = (count / summary['total_pilgrims'] * 100)
            print(f"   {nat}: {count:,} ({percentage:.1f}%)")
        print()
        
        # 6. Ø­ÙØ¸ Ø§Ù„ØªÙ‚Ø±ÙŠØ±
        print("ğŸ’¾ Step 5: Exporting report...")
        platform.export_report(report, '/home/claude/hajj_analysis_report.json')
        print()
        
        print("=" * 60)
        print("âœ… Analysis completed successfully!")
        print("=" * 60)
        
    except Exception as e:
        logger.error(f"âŒ Error during execution: {e}")
        raise
    
    finally:
        platform.cleanup()


if __name__ == "__main__":
    main()
